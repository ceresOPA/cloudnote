# 神经网络

> 学习来源：[马少平老师的个人空间\_哔哩哔哩_bilibili](https://space.bilibili.com/1000083494)

## 一、数字识别

### 1. 模式匹配

![image-20220304231555646](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304231555646.png)

- 输入*模板，根据匹配值net，值越大，可能性也越大

- 奖励+惩罚
- 存在的问题：仅仅根据net值并不一定能准确判断，因为比如1的笔画数很少，那1正确匹配的情况下得到的net值也很小。而对于笔画数多的数字来说，比如模式9，我输入一个1，去匹配模式9，也能够得到一个和直接匹配模式1，差不多的net值，那这就无从判断。也就是说笔画数越多的模式，越有优势，这样的net值比较就会是不合理的。

### 2. Sigmoid函数

![image-20220304233134508](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304233134508.png)

- 选更接近1的
- 但当有两个数都很大的时候，都接近1，就没法判断。此时，就可以增加一个偏执项，可以认为是一个阈值，进行一个偏移。

### 3. 什么是神经元

- 神经元，一种模式

### 4. 神经网络

![image-20220304234632287](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304234632287.png)

- 由多种神经元所构成的网络
- 不同神经元的权值(包括偏置值)即对应着不同的数字模式

### 5. 神经网络的横向扩展——增加模式

![image-20220304235133180](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304235133180.png)

- 增加模式
- 比如有各种长得不一样的3的写法，那就可以去给3增加各种长得不一样的模式，增加神经元。

### 6. 神经网络的纵向扩展——局部模式

![image-20220304235321687](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304235321687.png)

- 将原来的模式进行拆分，这样的话，相同的局部就可以共用了！

![image-20220304235452277](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220304235452277.png)

- 越靠近输入的力度越小，越靠近输出力度越大
- 输入为每个像素点，中间为匹配的局部范围内的像素点，不断往后，就可以组成一个全局的模式！
- 根据匹配的程度，匹配的好的神经元，就激活它，让他更有说话的份量。

### 7. 多层神经网络

- 每一层计算都是一样的
- 一层一层加深
- **越靠近输入层的神经元，刻画的模式越细致，体现的越是细微信息的特征；越是靠近输出层的神经元，刻画的模式越是体现了整体信息的特征。这样通过<font color="red">不同层次</font>的神经元体现的是<font color="red">不同粒度</font>的特征**

### 8. 如何获得模式

- 模式通过神经元的连接权重表示（w1、w2、w3、、、b）
- 通过训练样本，自动学习权重，也就是模式（如BP算法）
- 学习到的模式是一种隐含表达。

### 9. 总结

- **模式匹配**：每一个神经元对应的权值表示一种模式，模式匹配的过程就是输入与当前神经元的权值的加权和，再通过sigmoid函数转换为匹配上的概率，越大说明匹配的越好，则越有可能是正确的。
- **横向与纵向扩展**：横向的扩展是增加同一层相同粒度的模式种类的扩展，纵向的扩展是增加不同层不同粒度特征或说是不同粒度局部模式的扩展。

## 二、神经元与全连接网络

### 1. 神经元

![image-20220305113044884](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305113044884.png)

- **根据输入**，通过与权重的加权和后，再经过激活函数，**得到输出**

### 2. 激活函数

#### 符号函数（sign函数）

![image-20220305151455996](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305151455996.png)

- 最早的激活函数，像单层感知机中就会用到

#### Sigmoid函数

![image-20220305151444350](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305151444350.png)

- 输出0~1的范围
- 连续的，可作为概率

#### 双曲正切函数（tanh函数）

![image-20220305151419727](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305151419727.png)

- 和sigmoid函数很像，但是范围不同，范围为-1~1

#### 线性整流函数（ReLu函数）

![image-20220305151402533](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305151402533.png)

- 整流，像二极管一样，单向通过，即整流
- 经常用于图像处理的问题中

#### Softmax函数

![image-20220305151339396](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305151339396.png)

- 一般只用于最后的输出层
- 对输出层的各个神经元的输出的net(加权和)一起通过一个softmax函数，得到各个不同的概率。

### 3. 全连接网络

![image-20220305113120109](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220305113120109.png)

- 按神经元的连接方式，两层的神经元，两两之间都有连接，每个连接都有权重。
- 按信息走的方向来说，一层一层往前走，前一层的输出，作为下一层的输入，即**前馈网络**。

- **多层感知机**MLP，就是全连接网络，一开始都是说感知机的。
- **全连接层**，或**稠密层**，都是指全连接。

## 三、神经网络的训练

- 收集数据集
- 数据标注
- 训练集(平常作业)与测试集(期中期末考试)
  - 样本

- 损失函数-评价调节效果
  - 误差平方和
    - 为什么用平方
    - 为什么前面乘1/2
- 梯度下降法
  - 迭代逼近
  - 两个问题
    - 修改量的大小（learning_rate）
    - 修改的方向
  - $\theta_{i+1} = \theta_{i}-\eta\frac{df}{d\theta}$
  - 步长，解决振荡问题
  - 三种方案
    - 批量梯度下降法
      - 拿全部的样本训练，计算量大
    - 随机梯度下降法
      - 每次处理一个样本，更新比较快，但可能存在有标注错的样本，对训练操作影响
      - 梯度是由一个样本计算得到的，并不能代表所有样本的梯度方向
    - 小批量梯度下降法
      - 折中的方案
- 反向传播算法（BP：Back Propagation）
  - ![image-20220306151143290](https://gitee.com/y255413580/img/raw/master/noteimg/image-20220306151143290.png)
  - 又称误差反向传播算法（最小化误差）
  - 给出一种计算偏导的方法
  - BP算法会因具体条件不同，算法会有差异，**总体思想一样**
    - 上述BP算法的条件：
      - 全连接网络
      - 随机梯度下降法
      - 激活函数：sigmoid
      - 损失函数；误差平方和
- 交叉熵损失函数
  - 概率之和为1；概率在0~1之间

## 四、卷积神经网络

